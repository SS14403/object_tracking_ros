#!/usr/bin/env python3

import rospy
import numpy as np
import cv2 # Needed for visualization
from cv_bridge import CvBridge # Needed for visualization
from sensor_msgs.msg import Image # Needed for visualization
from application_util import preprocessing
# --- DeepSORT Imports ---
# Ensure the DeepSORT library path is correct and accessible
import sys
# ** ADJUST THIS PATH if deep_sort is cloned elsewhere **
DEEPSORT_PATH = '/home/shehab/ros_ws/src/perception_stack/deep_sort'
try:
    sys.path.append(DEEPSORT_PATH)
    from deep_sort.application_util import preprocessing
    from deep_sort.deep_sort import nn_matching
    from deep_sort.deep_sort.detection import Detection as DeepSortDetection # Rename to avoid conflict with our msg
    from deep_sort.deep_sort.tracker import Tracker
    rospy.loginfo(f"Successfully imported DeepSORT modules from: {DEEPSORT_PATH}")
except ImportError as e:
    rospy.logfatal(f"Failed to import DeepSORT modules. Check DEEPSORT_PATH: {DEEPSORT_PATH}. Error: {e}")
    rospy.signal_shutdown("DeepSORT import failed")
    # Exit if import fails
    sys.exit(1) # Or raise an exception that prevents node init

# --- ROS Custom Message Imports ---
# Import your custom messages from the perception_stack package
try:
    # Detections message from YOLO
    from perception_stack.msg import Detections
    # Your custom Detection message (might be reused in TrackedObjects)
    from perception_stack.msg import Detection as PerceptionDetection # Rename to avoid conflict
    # Output message containing tracked info
    from perception_stack.msg import TrackedObjects
except ImportError as e:
    rospy.logfatal(f"Failed to import custom perception_stack messages. Did you run catkin_make and source devel/setup.bash? Error: {e}")
    rospy.signal_shutdown("Custom message import failed")
    sys.exit(1)


class ObjectTracker:
    def __init__(self):
        rospy.init_node('object_tracker', anonymous=True)

        # --- DeepSORT Parameters ---
        # Get parameters from ROS param server or use defaults
        max_cosine_distance = rospy.get_param('~max_cosine_distance', 0.4)
        nn_budget = rospy.get_param('~nn_budget', None) # None for unlimited budget
        self.nms_max_overlap = rospy.get_param('~nms_max_overlap', 0.85) # High overlap threshold, assuming detector NMS is primary
        max_age = rospy.get_param('~max_age', 30) # Frames to keep unmatched track
        n_init = rospy.get_param('~n_init', 3)   # Frames to confirm a track

        # Path to the feature extraction model
        # ** ADJUST PATH if mars-small128.pb is located differently **
        model_filename = f'{DEEPSORT_PATH}/deep_sort/model_data/mars-small128.pb'
        rospy.loginfo(f"Attempting to load DeepSORT feature model from: {model_filename}")

        # Initialize DeepSORT components
        try:
            # Cosine distance metric for appearance features
            self.metric = nn_matching.NearestNeighborDistanceMetric("cosine", max_cosine_distance, nn_budget)
            # The main tracker object
            self.tracker = Tracker(self.metric, max_age=max_age, n_init=n_init)
            rospy.loginfo("DeepSORT Tracker initialized successfully.")
            rospy.loginfo(f"Params: Max Cosine Dist={max_cosine_distance}, NN Budget={nn_budget}, NMS Overlap={self.nms_max_overlap}, Max Age={max_age}, N Init={n_init}")
        except Exception as e:
            rospy.logfatal(f"Failed to initialize DeepSORT tracker components: {e}")
            rospy.signal_shutdown("DeepSORT initialization failed")
            return # Stop initialization

        # --- ROS Communication ---
        # Subscriber to YOLO detections
        self.detection_sub = rospy.Subscriber('/yolo_detections', Detections, self.detections_callback, queue_size=5)

        # Publisher for tracked objects message
        self.tracked_objects_pub = rospy.Publisher('/tracked_objects', TrackedObjects, queue_size=10)

        # --- Optional: For Visualization ---
        self.bridge = CvBridge()
        # Subscribe to the original camera stream to draw on
        self.image_sub = rospy.Subscriber('/camera_stream', Image, self.image_callback, queue_size=1, buff_size=2**24)
        # Publisher for the visualization image
        self.tracking_vis_pub = rospy.Publisher('/tracking_visualization', Image, queue_size=10)
        self.current_frame = None # Store the latest frame
        self.last_published_tracks = None # Store last tracks to draw

        rospy.loginfo("Object Tracker Node Initialized and Waiting for Detections.")

    def image_callback(self, msg):
        """ Stores the latest image frame for visualization """
        try:
            self.current_frame = self.bridge.imgmsg_to_cv2(msg, 'bgr8')
            # We call visualize_tracking() after processing detections to ensure drawing uses latest tracks
        except Exception as e:
            rospy.logerr(f"Image callback error in tracker: {e}")

    def detections_callback(self, detections_msg):
        """ Processes detections from YOLO, updates tracker, and publishes results """
        if not hasattr(self, 'tracker') or self.current_frame is None:
             rospy.logwarn_throttle(5, "Tracker not initialized or frame not received yet. Skipping detection.")
             return

        header = detections_msg.header
        perception_detections = detections_msg.detections # List of PerceptionDetection messages

        if not perception_detections:
            # No detections received, predict tracker state and publish empty tracks
            rospy.logdebug("No detections received. Predicting tracker state.")
            self.tracker.predict()
            # Publish empty message or just don't publish? Publishing empty is often better.
            empty_tracks_msg = TrackedObjects(header=header)
            self.tracked_objects_pub.publish(empty_tracks_msg)
            self.last_published_tracks = empty_tracks_msg # Store for visualization
            self.visualize_tracking() # Update visualization
            return

        # --- 1. Convert perception_stack/Detections to DeepSORT format ---
        # DeepSORT typically needs: Top-Left-Width-Height (TLWH) bounding boxes and confidence scores.
        bboxes_tlwh = []    # List of [x, y, w, h] for DeepSORT
        confidences = []    # List of confidences
        class_names = []    # List of class names corresponding to boxes

        for det in perception_detections:
            # Convert center_x, center_y, width, height to top-left_x, top-left_y, width, height
            tl_x = det.x - det.w / 2.0
            tl_y = det.y - det.h / 2.0
            bboxes_tlwh.append([tl_x, tl_y, det.w, det.h])
            confidences.append(det.confidence)
            class_names.append(det.class_name)

        bboxes_tlwh_np = np.array(bboxes_tlwh)
        confidences_np = np.array(confidences)

        # --- 2. Optional: Non-Max Suppression (helps if detector's NMS is insufficient) ---
        # indices = preprocessing.non_max_suppression(bboxes_tlwh_np, self.nms_max_overlap, confidences_np)
        # bboxes_tlwh_np = bboxes_tlwh_np[indices]
        # confidences_np = confidences_np[indices]
        # class_names = [class_names[i] for i in indices] # Filter corresponding class names

        # --- 3. Feature Extraction (Crucial for DeepSORT's appearance matching) ---
        # Extract image patches based on detections and get appearance features.
        # The 'current_frame' received by image_callback is needed here.
        features = self._get_features(bboxes_tlwh_np, self.current_frame)

        # --- 4. Create DeepSORT Detection objects ---
        # Combine bbox, confidence, class name, and features
        deep_sort_detections = []
        for i in range(len(bboxes_tlwh_np)):
             bbox = bboxes_tlwh_np[i]
             conf = confidences_np[i]
             cls_name = class_names[i]
             feat = features[i]
             # Create DeepSORT's specific Detection object
             deep_sort_detections.append(DeepSortDetection(bbox, conf, cls_name, feat))

        # --- 5. Update the Tracker ---
        self.tracker.predict() # Predict next state based on motion model
        self.tracker.update(deep_sort_detections) # Update with current detections and features

        # --- 6. Process Tracker Output and Publish ---
        tracked_objects_msg = TrackedObjects()
        tracked_objects_msg.header = header # Use header from detections message

        # Clear lists before populating
        tracked_objects_msg.ids = []
        tracked_objects_msg.classes = []
        if hasattr(tracked_objects_msg, 'tracked_detections'): # Check if field exists
             tracked_objects_msg.tracked_detections = []

        active_tracks = [] # Keep track of tracks for visualization
        for track in self.tracker.tracks:
            # Filter out tentative tracks or those lost for too long
            if not track.is_confirmed() or track.time_since_update > 1:
                continue

            active_tracks.append(track) # Add to list for visualization

            # Get track ID and current estimated bounding box (TLWH format)
            track_id = track.track_id
            bbox_tlwh = track.to_tlwh() # Get filtered/smoothed bounding box
            class_name = track.get_class() # Get originally associated class name

            # Populate the TrackedObjects message
            tracked_objects_msg.ids.append(track_id)
            tracked_objects_msg.classes.append(class_name)

            # --- Populate bounding box info if TrackedObjects.msg supports it ---
            # (Highly recommended for the optical flow node later)
            if hasattr(tracked_objects_msg, 'tracked_detections'):
                tracked_detection = PerceptionDetection()
                # Convert TLWH back to center x, y, w, h (or your msg format)
                tracked_detection.x = bbox_tlwh[0] + bbox_tlwh[2] / 2.0
                tracked_detection.y = bbox_tlwh[1] + bbox_tlwh[3] / 2.0
                tracked_detection.w = bbox_tlwh[2]
                tracked_detection.h = bbox_tlwh[3]
                tracked_detection.confidence = 1.0 # Confidence high for confirmed tracks
                tracked_detection.class_name = class_name
                # Add track ID if your PerceptionDetection msg has an ID field:
                # if hasattr(tracked_detection, 'id'):
                #     tracked_detection.id = track_id
                tracked_objects_msg.tracked_detections.append(tracked_detection)
            # --------------------------------------------------------------------

        # Publish the message with tracked object info
        self.tracked_objects_pub.publish(tracked_objects_msg)
        self.last_published_tracks = tracked_objects_msg # Store for visualization

        # Call visualization update
        self.visualize_tracking(active_tracks)


    def _get_features(self, tlwh_bboxes, frame):
        """ Extracts appearance features for each bounding box using DeepSORT's model. """
        # DeepSORT's preprocessing often expects BGR images.
        image_patches = []
        for bbox in tlwh_bboxes:
            x, y, w, h = map(int, bbox)
            # Ensure box coordinates are valid and within frame bounds
            x = max(0, x)
            y = max(0, y)
            x2 = min(frame.shape[1], x + w)
            y2 = min(frame.shape[0], y + h)
            # Check if the ROI is valid
            if x2 > x and y2 > y:
                 patch = frame[y:y2, x:x2]
                 image_patches.append(patch)
            else:
                 # Append a dummy value or handle error if bbox is invalid
                 # Appending a small black patch as a fallback
                 rospy.logwarn_throttle(10, f"Invalid bounding box ROI encountered: [{x},{y},{w},{h}] clipped to [{x},{y},{x2-x},{y2-y}]")
                 image_patches.append(np.zeros((1, 1, 3), dtype=frame.dtype))


        # Use DeepSORT's feature extractor (modify if using a different extractor)
        # This part might need adjustment depending on the exact DeepSORT implementation
        # you cloned. It might have a dedicated feature extractor class.
        # Assuming a function `extract_features(patches)` exists or needs to be adapted:
        if image_patches:
             # Placeholder: Replace with actual feature extraction call if available
             # features = self.feature_extractor.extract(image_patches)
             # If no explicit extractor, return empty features (relies on motion only)
             features = np.array([[] for _ in image_patches]) # Return empty features per patch
             # --- OR ---
             # If the standard DeepSORT repo expects direct encoding (less common):
             # try:
             #     features = self.encoder(frame, bboxes_tlwh_np) # Example hypothetical call
             # except AttributeError:
             #     rospy.logwarn_throttle(10,"DeepSORT feature extraction method not found/adapted. Using motion only.")
             #     features = np.array([[] for _ in image_patches])
        else:
            features = np.array([])

        # Return features matching the order of input bboxes
        # Should be shape (num_bboxes, feature_dim) or list of arrays
        return features


    def visualize_tracking(self, active_tracks=None):
         """ Draws bounding boxes and IDs on the current frame. """
         if self.current_frame is None:
             return # Cannot visualize without a frame

         vis_frame = self.current_frame.copy()

         # Use provided active_tracks if available (more efficient)
         tracks_to_draw = active_tracks
         header = None

         # If not provided, get from last published message (less efficient)
         if tracks_to_draw is None and self.last_published_tracks:
              header = self.last_published_tracks.header
              # Rebuild track info from message if needed (less ideal)
              # This part depends heavily on TrackedObjects.msg structure
              # If it has tracked_detections:
              if hasattr(self.last_published_tracks, 'tracked_detections'):
                   temp_tracks = []
                   for i, track_id in enumerate(self.last_published_tracks.ids):
                        det = self.last_published_tracks.tracked_detections[i]
                        # Convert back to TLWH for drawing consistency
                        bbox_tlwh = [det.x - det.w/2.0, det.y - det.h/2.0, det.w, det.h]
                        # Create a temporary structure or tuple for drawing
                        temp_tracks.append({'id': track_id, 'bbox': bbox_tlwh, 'class': det.class_name})
                   tracks_to_draw = temp_tracks
              else: # Cannot draw boxes if only IDs/Classes are published
                   tracks_to_draw = [] # Or just draw IDs without boxes
                   rospy.logwarn_throttle(10,"Cannot visualize bounding boxes as they are not in TrackedObjects message.")


         if tracks_to_draw:
             for track in tracks_to_draw:
                 # Adapt based on whether track is a Tracker object or dict from message
                 if isinstance(track, Tracker): # From tracker update
                     bbox_tlwh = track.to_tlwh()
                     track_id = track.track_id
                     class_name = track.get_class()
                 elif isinstance(track, dict): # From rebuilt message info
                     bbox_tlwh = track['bbox']
                     track_id = track['id']
                     class_name = track['class']
                 else: continue # Skip if format is unexpected

                 # Convert tlwh to xmin, ymin, xmax, ymax for drawing
                 xmin = int(bbox_tlwh[0])
                 ymin = int(bbox_tlwh[1])
                 xmax = int(bbox_tlwh[0] + bbox_tlwh[2])
                 ymax = int(bbox_tlwh[1] + bbox_tlwh[3])

                 # Draw bounding box (e.g., blue for tracked objects)
                 cv2.rectangle(vis_frame, (xmin, ymin), (xmax, ymax), (255, 0, 0), 2)
                 label = f"ID: {track_id} ({class_name})"
                 cv2.putText(vis_frame, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2) # Blue text

         try:
              vis_msg = self.bridge.cv2_to_imgmsg(vis_frame, 'bgr8')
              # Use header from detections if possible, otherwise use current time
              if header:
                   vis_msg.header = header
              else:
                   vis_msg.header.stamp = rospy.Time.now()
                   # vis_msg.header.frame_id = ... # Set frame_id if known
              self.tracking_vis_pub.publish(vis_msg)
         except Exception as e:
              rospy.logerr(f"CV Bridge error during tracking visualization: {e}")


    def run(self):
        rospy.spin()

if __name__ == '__main__':
    try:
        tracker_node = ObjectTracker()
        # Check if tracker initialization was successful before spinning
        if hasattr(tracker_node, 'tracker'):
             tracker_node.run()
        else:
            # Error already logged during __init__
            pass
    except rospy.ROSInterruptException:
        rospy.loginfo("Object Tracker node shutting down.")
    except Exception as e:
        rospy.logfatal(f"Unhandled exception in Object Tracker main: {e}")
